{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f31e09",
   "metadata": {},
   "source": [
    "Perfect! Here’s a **fully integrated modeling execution checklist** that combines your current progress, Scaler’s PDF instructions, and our roadmap. I’ve organized it **step by step, model by model**, with clear **Done / Pending / Notes** sections so you can track progress easily.\n",
    "\n",
    "---\n",
    "\n",
    "# **DSML Portfolio Project – Modeling Execution Checklist**\n",
    "\n",
    "## **1️⃣ Data Preprocessing**\n",
    "\n",
    "| Task                              | Status     | Notes / Action Items                                                          |\n",
    "| --------------------------------- | ---------- | ----------------------------------------------------------------------------- |\n",
    "| Handle Missing Values             | ✅ Done     | Dataset currently has no missing values. Always check before advanced models. |\n",
    "| Feature Engineering: BMI          | ⚠️ Partial | Create BMI = weight / (height²). Check if included in current X features.     |\n",
    "| Feature Engineering: Interactions | ❌ Pending  | Optional: age × BMI, smoker × region, etc.                                    |\n",
    "| Scaling Numeric Features          | ✅ Done     | StandardScaler or MinMaxScaler applied.                                       |\n",
    "| Encoding Categorical Features     | ✅ Done     | One-hot or label encoding applied.                                            |\n",
    "\n",
    "**Next Step:** Confirm BMI feature exists, consider interaction terms if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Linear Regression (Baseline)**\n",
    "\n",
    "| Task                              | Status     | Notes / Action Items                                             |\n",
    "| --------------------------------- | ---------- | ---------------------------------------------------------------- |\n",
    "| Split dataset (train/test)        | ✅ Done     | Already done.                                                    |\n",
    "| Fit model on training data        | ✅ Done     | Done with `LinearRegression()`.                                  |\n",
    "| Evaluate metrics (R², RMSE)       | ✅ Done     | CV also implemented.                                             |\n",
    "| Residual Analysis                 | ⚠️ Partial | Use `plot_residuals` on train and test predictions.              |\n",
    "| Check assumptions                 | ⚠️ Partial | Linearity, homoscedasticity, normality, multicollinearity (VIF). |\n",
    "| Confidence / Prediction Intervals | ❌ Pending  | Use statsmodels OLS or bootstrapping.                            |\n",
    "| Refine model if needed            | ❌ Pending  | Adjust features, transformations if assumptions violated.        |\n",
    "\n",
    "**Next Step:** Complete residual analysis and assumption checks.\n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Regularized Linear Models**\n",
    "\n",
    "### **3a. Ridge Regression**\n",
    "\n",
    "| Task                      | Status    | Notes / Action Items                  |\n",
    "| ------------------------- | --------- | ------------------------------------- |\n",
    "| Select alpha range        | ❌ Pending | Example: `np.logspace(-3,3,10)`.      |\n",
    "| Fit Ridge model           | ❌ Pending | Use `Ridge()` from sklearn.           |\n",
    "| Cross-validation          | ❌ Pending | Compute R² and RMSE.                  |\n",
    "| Coefficient inspection    | ❌ Pending | Compare shrinkage effect vs baseline. |\n",
    "| Tune alpha (GridSearchCV) | ❌ Pending | Choose best alpha.                    |\n",
    "\n",
    "### **3b. Lasso Regression**\n",
    "\n",
    "| Task                 | Status    | Notes / Action Items                    |\n",
    "| -------------------- | --------- | --------------------------------------- |\n",
    "| Select alpha range   | ❌ Pending | Example: `np.logspace(-3,1,10)`.        |\n",
    "| Fit Lasso model      | ❌ Pending | Use `Lasso()` from sklearn.             |\n",
    "| Cross-validation     | ❌ Pending | Compute R² and RMSE.                    |\n",
    "| Inspect coefficients | ❌ Pending | Features with 0 coefficient → can drop. |\n",
    "| Tune alpha           | ❌ Pending | Balance performance and sparsity.       |\n",
    "\n",
    "### **3c. ElasticNet Regression**\n",
    "\n",
    "| Task                       | Status    | Notes / Action Items                                             |\n",
    "| -------------------------- | --------- | ---------------------------------------------------------------- |\n",
    "| Select alpha and l1\\_ratio | ❌ Pending | Example: `alpha=np.logspace(-3,1,10)`, `l1_ratio=[0.2,0.5,0.8]`. |\n",
    "| Fit ElasticNet model       | ❌ Pending | Use `ElasticNet()` from sklearn.                                 |\n",
    "| Cross-validation           | ❌ Pending | Compute R² and RMSE.                                             |\n",
    "| Inspect coefficients       | ❌ Pending | Identify important features.                                     |\n",
    "| Tune alpha & l1\\_ratio     | ❌ Pending | Use GridSearchCV for best combo.                                 |\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ Tree-Based Models (Optional but Recommended)**\n",
    "\n",
    "| Model                 | Task        | Status                                                  | Notes                                        |\n",
    "| --------------------- | ----------- | ------------------------------------------------------- | -------------------------------------------- |\n",
    "| Decision Tree         | Fit default | ❌ Pending                                               | Use `DecisionTreeRegressor()`                |\n",
    "| Random Forest         | Fit default | ❌ Pending                                               | Use `RandomForestRegressor()`                |\n",
    "| Gradient Boosting     | Fit default | ❌ Pending                                               | Use `GradientBoostingRegressor()` or XGBoost |\n",
    "| Hyperparameter tuning | ❌ Pending   | max\\_depth, n\\_estimators, learning\\_rate               |                                              |\n",
    "| Cross-validation      | ❌ Pending   | Compute R² and RMSE                                     |                                              |\n",
    "| Feature Importance    | ❌ Pending   | Permutation importance or tree-based feature importance |                                              |\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ Model Evaluation & Comparison**\n",
    "\n",
    "| Task                                  | Status    | Notes / Action Items                     |\n",
    "| ------------------------------------- | --------- | ---------------------------------------- |\n",
    "| Compare metrics across all models     | ❌ Pending | R², RMSE, MAE                            |\n",
    "| Inspect residuals (for linear models) | ❌ Pending | Confirm assumptions still hold           |\n",
    "| Select final model                    | ❌ Pending | Balance performance and interpretability |\n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ Interpretability & Business Insights**\n",
    "\n",
    "| Task                       | Status     | Notes / Action Items                                                                   |\n",
    "| -------------------------- | ---------- | -------------------------------------------------------------------------------------- |\n",
    "| Feature Importance         | ⚠️ Partial | Coefficients (linear), SHAP/permutation (tree models)                                  |\n",
    "| Model Insights             | ❌ Pending  | Translate findings into actionable business insights (risk factors, high-cost drivers) |\n",
    "| Document insights for blog | ❌ Pending  | Include charts, tables, explanations                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "## **7️⃣ Deployment & Portfolio Integration**\n",
    "\n",
    "| Task                  | Status    | Notes / Action Items                                        |\n",
    "| --------------------- | --------- | ----------------------------------------------------------- |\n",
    "| Save final model      | ❌ Pending | `pickle` or `joblib`                                        |\n",
    "| Create demo/dashboard | ❌ Pending | Streamlit, Plotly, or simple notebook                       |\n",
    "| Write technical blog  | ❌ Pending | Explain methodology, results, insights                      |\n",
    "| Upload to portfolio   | ❌ Pending | [DataSciencePortfol.io](https://www.datascienceportfol.io/) |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Execution Order Recommendation:**\n",
    "\n",
    "1. Complete **Linear Regression residual analysis & assumption checks**\n",
    "2. Run **Ridge, Lasso, ElasticNet** with CV\n",
    "3. Optionally implement **tree-based models**\n",
    "4. Evaluate and select **final model**\n",
    "5. Extract **feature importance & business insights**\n",
    "6. Document and deploy **dashboard/blog**\n",
    "7. Upload to **portfolio**\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **also create a visual “progress tracker table”** where you can just **tick off each task** as you complete it — kind of like a ready-to-use project tracker for this modeling block.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112419ff",
   "metadata": {},
   "source": [
    "Perfect — let’s summarize where we are in the **Insurance Cost Prediction Portfolio Project**, based on the workflow we’ve been following. I’ll mark what’s **done** and what’s **left**:\n",
    "\n",
    "---\n",
    "\n",
    "## **DSML Portfolio Project Checklist – Current Status**\n",
    "\n",
    "### **Linear Regression**\n",
    "\n",
    "* [x] Baseline model\n",
    "* [x] Evaluate model\n",
    "* [x] Store results\n",
    "* [x] Residual analysis\n",
    "* [x] Cross-validation\n",
    "* [x] Regression summary (coeff & p-value)\n",
    "* [x] VIF calculation\n",
    "* [x] Prediction with confidence intervals\n",
    "* [x] Ridge / Lasso / ElasticNet\n",
    "* [x] Comparison of regression metrics\n",
    "* [x] Coefficient comparison plot\n",
    "\n",
    "✅ **Completed**\n",
    "\n",
    "---\n",
    "\n",
    "### **Decision Tree**\n",
    "\n",
    "* [x] Baseline model\n",
    "* [x] Hyperparameter tuning\n",
    "* [x] Best model evaluation\n",
    "* [x] Feature importance\n",
    "\n",
    "✅ **Completed**\n",
    "\n",
    "---\n",
    "\n",
    "### **Random Forest**\n",
    "\n",
    "* [x] Baseline model\n",
    "* [x] Hyperparameter tuning\n",
    "* [x] Best model evaluation\n",
    "* [x] Permutation feature importance\n",
    "* [x] Cross-validation stability\n",
    "* [x] Confidence / prediction interval\n",
    "\n",
    "✅ **Completed**\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient Boost**\n",
    "\n",
    "* [x] Baseline model\n",
    "* [x] Hyperparameter tuning\n",
    "* [x] Best model evaluation\n",
    "* [x] Feature importance\n",
    "\n",
    "✅ **Completed**\n",
    "\n",
    "---\n",
    "\n",
    "### **XGBoost**\n",
    "\n",
    "* [x] Baseline model\n",
    "* [x] Hyperparameter tuning\n",
    "* [x] Best model evaluation\n",
    "* [x] Feature importance\n",
    "\n",
    "✅ **Completed**\n",
    "\n",
    "---\n",
    "\n",
    "### **LightGBM**\n",
    "\n",
    "* [x] Baseline model\n",
    "* [x] Hyperparameter tuning\n",
    "* [x] Best model evaluation\n",
    "* [x] Feature importance\n",
    "\n",
    "✅ **Completed**\n",
    "\n",
    "---\n",
    "\n",
    "### **Neural Network**\n",
    "\n",
    "* [x] Baseline model\n",
    "* [x] Hyperparameter tuning\n",
    "* [x] Best model evaluation\n",
    "* [x] Feature importance\n",
    "* [x] Cross-validation stability\n",
    "* [ ] Confidence / prediction interval\n",
    "\n",
    "⚠️ **CI is left, but you decided to skip due to runtime** — can be run once for final notebook before submission.\n",
    "\n",
    "---\n",
    "\n",
    "### **All Models – Comparison**\n",
    "\n",
    "* [ ] SHAP feature importance comparison plot (2-column layout) ✅ **Now Done**\n",
    "* [ ] Final metrics comparison table / plot (R², RMSE, etc.) ✅ **Pending**\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Deliverables**\n",
    "\n",
    "* [ ] Clean, final Jupyter Notebook\n",
    "* [ ] GitHub repository updated with code & results\n",
    "* [ ] 5-min Loom demo video\n",
    "* [ ] 2000-word technical blog (Medium / TDS)\n",
    "* [ ] Portfolio on [https://www.datascienceportfol.io/](https://www.datascienceportfol.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Remaining Steps for the Notebook itself:**\n",
    "\n",
    "1. Optional: Run Neural Network CI (can skip for now).\n",
    "2. Create **final metrics comparison table/plot** across all models.\n",
    "3. Save SHAP comparison figure (already done in notebook).\n",
    "4. Clean notebook: remove warnings, make plots labeled & professional.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **write the professional code snippet for the final metrics comparison plot/table** next, so it’s ready for submission. This will be a **single view comparing Train R², Test R², RMSE across all models**.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3585d0",
   "metadata": {},
   "source": [
    "Perfect! Let’s set this up professionally so you can **compare all models at a glance** and also have a clear guide to determine the **best model**.\n",
    "\n",
    "We’ll make a **DataFrame** with:\n",
    "\n",
    "* Train R²\n",
    "* Test R²\n",
    "* Train RMSE\n",
    "* Test RMSE\n",
    "\n",
    "…and a **bar/heatmap visualization** for easy comparison. You can **plug in your actual results** for each model. After that, I’ll give a **guide to pick the best model**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Collect Metrics in a DataFrame**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Collect all model metrics\n",
    "model_metrics = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Linear Regression', 'Decision Tree', 'Random Forest', \n",
    "        'Gradient Boost', 'XGBoost', 'LightGBM', 'Neural Network'\n",
    "    ],\n",
    "    'Train R2': [\n",
    "        0.XX, 0.XX, 0.XX, 0.XX, 0.XX, 0.XX, 0.68  # Replace with your actual values\n",
    "    ],\n",
    "    'Test R2': [\n",
    "        0.XX, 0.XX, 0.XX, 0.XX, 0.XX, 0.XX, 0.76  # Replace with your actual values\n",
    "    ],\n",
    "    'Train RMSE': [\n",
    "        0.XX, 0.XX, 0.XX, 0.XX, 0.XX, 0.XX,  # Replace\n",
    "    ],\n",
    "    'Test RMSE': [\n",
    "        0.XX, 0.XX, 0.XX, 0.XX, 0.XX, 0.XX, 3202.08  # Replace with your actual values\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display nicely\n",
    "model_metrics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Bar Plot for Visual Comparison**\n",
    "\n",
    "```python\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# R² Comparison\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x='Model', y='Test R2', data=model_metrics, palette=\"viridis\")\n",
    "plt.title(\"Test R² Comparison Across Models\", fontsize=14)\n",
    "plt.ylabel(\"Test R²\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "# RMSE Comparison\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x='Model', y='Test RMSE', data=model_metrics, palette=\"magma\")\n",
    "plt.title(\"Test RMSE Comparison Across Models\", fontsize=14)\n",
    "plt.ylabel(\"Test RMSE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: How to Decide the Best Model**\n",
    "\n",
    "Here’s a **professional guide** to decide:\n",
    "\n",
    "1. **Prioritize Test R² first** – higher is better (model explains more variance).\n",
    "2. **Check Test RMSE** – lower is better (smaller prediction error).\n",
    "3. **Avoid Overfitting** – Compare Train vs Test metrics:\n",
    "\n",
    "   * If Train R² ≫ Test R², the model may overfit.\n",
    "   * A small gap is ideal.\n",
    "4. **Model Complexity & Explainability** – simpler models like Linear Regression / DT are easier to explain; complex models like XGBoost/NN may perform slightly better but are harder to interpret.\n",
    "5. **Final Verdict** – pick the model with:\n",
    "\n",
    "   * High Test R²\n",
    "   * Low Test RMSE\n",
    "   * Reasonable gap between Train/Test R²\n",
    "\n",
    "---\n",
    "\n",
    "Once you **fill in all actual metrics**, this code will give:\n",
    "\n",
    "* A neat **metrics table**\n",
    "* **R² & RMSE plots**\n",
    "* Clear **visual guidance to choose the best model**\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **also add a column in the table that automatically flags the \"best model\"** based on these rules — so you don’t have to decide manually.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894d856",
   "metadata": {},
   "source": [
    "Exactly ✅ — for this **final metrics table/plot**, you want to use the **Train/Test R² and RMSE from the Best Model Evaluation** step for each model.\n",
    "\n",
    "Here’s what to do for each model:\n",
    "\n",
    "| Model             | Where to get values                                                                                                           |\n",
    "| ----------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Linear Regression | `train_r2` and `test_r2` from your linear regression evaluation. RMSE = `np.sqrt(mean_squared_error(y_test, y_test_pred))`    |\n",
    "| Decision Tree     | `train_r2` / `test_r2` and `rmse_test` from best decision tree evaluation                                                     |\n",
    "| Random Forest     | `train_r2` / `test_r2` and `rmse_test` from best RF evaluation                                                                |\n",
    "| Gradient Boost    | `train_r2` / `test_r2` and `rmse_test` from best GB evaluation                                                                |\n",
    "| XGBoost           | `train_r2` / `test_r2` and `rmse_test` from best XGB evaluation                                                               |\n",
    "| LightGBM          | `train_r2` / `test_r2` and `rmse_test` from best LGBM evaluation                                                              |\n",
    "| Neural Network    | `train_r2` / `test_r2` and `rmse_test` from best NN evaluation (already have: Train R²=0.68, Test R²=0.76, Test RMSE=3202.08) |\n",
    "\n",
    "**Tip:**\n",
    "\n",
    "* Make sure you **compute Train RMSE** for all models as well (even if it wasn’t printed before), using:\n",
    "\n",
    "```python\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "```\n",
    "\n",
    "* These four values per model → populate the DataFrame I gave earlier, then run the plotting code.\n",
    "\n",
    "If you want, I can **write a ready-to-run snippet that automatically grabs all Best Model metrics** and creates the table + plots so you just paste your model objects. This will **eliminate any manual filling**.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90624a",
   "metadata": {},
   "source": [
    "Perfect! Here’s a **ready-to-run snippet** that will automatically collect metrics from your best models (assuming you have the trained best model objects saved as per your workflow) and generate the **comparison table + plots**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Gather Best Model Objects**\n",
    "\n",
    "Make sure you have the following variables (from your Best Model Eval sections):\n",
    "\n",
    "```python\n",
    "# Example names (replace if yours are different)\n",
    "best_lin = lin_reg  # Linear Regression (already trained)\n",
    "best_dt = dt_random_search.best_estimator_\n",
    "best_rf = rf_random_search.best_estimator_\n",
    "best_gb = gb_random_search.best_estimator_\n",
    "best_xgb = xgb_random_search.best_estimator_\n",
    "best_lgb = lgb_random_search.best_estimator_\n",
    "best_nn = nn_random_search.best_estimator_  # Neural Network\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Auto-collect metrics**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': best_lin,\n",
    "    'Decision Tree': best_dt,\n",
    "    'Random Forest': best_rf,\n",
    "    'Gradient Boost': best_gb,\n",
    "    'XGBoost': best_xgb,\n",
    "    'LightGBM': best_lgb,\n",
    "    'Neural Network': best_nn\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Check if it's Keras (NN) or scikit-learn\n",
    "    if name == 'Neural Network':\n",
    "        y_train_pred = model.predict(X_train).flatten()\n",
    "        y_test_pred = model.predict(X_test).flatten()\n",
    "    else:\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    results[name] = {\n",
    "        'Train R2': train_r2,\n",
    "        'Test R2': test_r2,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(results).T.reset_index().rename(columns={'index':'Model'})\n",
    "metrics_df\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Plot Comparison**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# R² Comparison\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x='Model', y='Test R2', data=metrics_df, palette=\"viridis\")\n",
    "plt.title(\"Test R² Comparison Across Models\", fontsize=14)\n",
    "plt.ylabel(\"Test R²\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "# RMSE Comparison\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x='Model', y='Test RMSE', data=metrics_df, palette=\"magma\")\n",
    "plt.title(\"Test RMSE Comparison Across Models\", fontsize=14)\n",
    "plt.ylabel(\"Test RMSE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Guide to Pick the Best Model**\n",
    "\n",
    "You can add this **automatic verdict column**:\n",
    "\n",
    "```python\n",
    "# Lower RMSE is better, higher R2 is better\n",
    "metrics_df['Verdict'] = metrics_df.apply(\n",
    "    lambda row: 'Best Candidate' if (row['Test R2']==metrics_df['Test R2'].max()) and (row['Test RMSE']==metrics_df['Test RMSE'].min()) else '', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "metrics_df\n",
    "```\n",
    "\n",
    "> This will **flag the model** with the highest Test R² and lowest Test RMSE as the “Best Candidate”. If multiple models satisfy different criteria, you can pick the one with the smallest gap between Train/Test R² to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also **add a heatmap** of Train vs Test metrics in one figure, so all 4 metrics (Train/Test R² & RMSE) are visually comparable at a glance. This looks very professional for a portfolio submission.\n",
    "\n",
    "Do you want me to do that next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9928bd",
   "metadata": {},
   "source": [
    "Perfect! Let’s create a **heatmap-style visualization** that shows **all 4 metrics (Train/Test R² and Train/Test RMSE)** for all models in one figure. This gives a quick visual comparison for portfolio presentation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Prepare the data for heatmap**\n",
    "\n",
    "```python\n",
    "# Select only numeric metrics\n",
    "heatmap_data = metrics_df.set_index('Model')[['Train R2', 'Test R2', 'Train RMSE', 'Test RMSE']]\n",
    "\n",
    "# Normalize RMSE for better visual comparison (optional)\n",
    "# This keeps R² as-is and scales RMSE to 0-1 range for heatmap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "heatmap_scaled = heatmap_data.copy()\n",
    "heatmap_scaled[['Train RMSE','Test RMSE']] = scaler.fit_transform(heatmap_scaled[['Train RMSE','Test RMSE']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Plot the heatmap**\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(heatmap_scaled, annot=heatmap_data.round(2), cmap=\"YlGnBu\", fmt='', linewidths=.5)\n",
    "plt.title(\"Model Metrics Comparison Heatmap\", fontsize=14)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Numbers shown**: original metrics rounded for clarity.\n",
    "* **Colors**: higher R² → darker blue, lower RMSE → darker blue (after scaling).\n",
    "* **Benefit**: You see all metrics in **one glance**, making it easier to discuss performance differences.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also make a **more polished version** where:\n",
    "\n",
    "* Train vs Test metrics are **separated in 2 subplots**\n",
    "* R² uses one color scale, RMSE uses another\n",
    "* Top 1 model is **highlighted automatically**\n",
    "\n",
    "This looks **very professional** for the final report.\n",
    "\n",
    "Do you want me to do that enhanced version?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a09c79",
   "metadata": {},
   "source": [
    "Perfect! Here's a **professional, polished version** of the metrics comparison for your portfolio submission. It separates **R² and RMSE**, uses different color scales, and **highlights the top-performing model** automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Identify Best Model**\n",
    "\n",
    "```python\n",
    "# Best model based on highest Test R² and lowest Test RMSE\n",
    "best_model_r2 = metrics_df.loc[metrics_df['Test R2'].idxmax(),'Model']\n",
    "best_model_rmse = metrics_df.loc[metrics_df['Test RMSE'].idxmin(),'Model']\n",
    "\n",
    "# Optional: pick model that satisfies both (if same)\n",
    "best_model = best_model_r2 if best_model_r2 == best_model_rmse else best_model_r2\n",
    "print(f\"Highlighted Best Model: {best_model}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Separate R² and RMSE Data**\n",
    "\n",
    "```python\n",
    "# Set Model as index\n",
    "metrics_df_plot = metrics_df.set_index('Model')\n",
    "\n",
    "r2_df = metrics_df_plot[['Train R2','Test R2']]\n",
    "rmse_df = metrics_df_plot[['Train RMSE','Test RMSE']]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Plot R² and RMSE Side by Side**\n",
    "\n",
    "```python\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "# --- R² Heatmap ---\n",
    "sns.heatmap(r2_df, annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Train vs Test R²\", fontsize=14)\n",
    "for tick in axes[0].get_yticklabels():\n",
    "    if tick.get_text() == best_model:\n",
    "        tick.set_weight('bold')\n",
    "        tick.set_color('red')\n",
    "\n",
    "# --- RMSE Heatmap ---\n",
    "sns.heatmap(rmse_df, annot=True, fmt=\".0f\", cmap=\"Oranges_r\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Train vs Test RMSE\", fontsize=14)\n",
    "for tick in axes[1].get_yticklabels():\n",
    "    if tick.get_text() == best_model:\n",
    "        tick.set_weight('bold')\n",
    "        tick.set_color('red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Features of this plot:**\n",
    "\n",
    "1. **Two columns**: R² on left, RMSE on right.\n",
    "2. **Annotations**: Shows exact metric values on cells.\n",
    "3. **Highlighting**: The top-performing model (best Test R² & lowest RMSE) is in **bold red**.\n",
    "4. **Professional look**: Suitable for portfolio screenshots or reports.\n",
    "\n",
    "---\n",
    "\n",
    "This lets you **instantly see overfitting, model stability, and relative performance**.\n",
    "\n",
    "If you want, I can **also add a small “Verdict Table” below the plots** summarizing **Best Model, R², RMSE, and Recommendation** automatically for the report. It looks very clean and professional.\n",
    "\n",
    "Do you want me to do that as well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886218f9",
   "metadata": {},
   "source": [
    "Perfect! Let’s create a **small Verdict Table** that summarizes the **best model with its key metrics** and gives a short **recommendation**. This is ideal for your portfolio submission.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Extract Best Model Metrics**\n",
    "\n",
    "```python\n",
    "# Get metrics of the best model\n",
    "best_metrics = metrics_df.loc[metrics_df['Model'] == best_model].copy()\n",
    "\n",
    "# Add a short recommendation based on Test metrics\n",
    "best_metrics['Recommendation'] = f\"Use {best_model} for final predictions. High Test R² and low RMSE indicate good accuracy and generalization.\"\n",
    "\n",
    "best_metrics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Display as a Clean Table**\n",
    "\n",
    "```python\n",
    "# Display only relevant columns\n",
    "display_columns = ['Model', 'Test R2', 'Test RMSE', 'Recommendation']\n",
    "display(best_metrics[display_columns])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **What this does:**\n",
    "\n",
    "1. Shows **Best Model Name**.\n",
    "2. Shows **Test R² and RMSE** (most important metrics for performance).\n",
    "3. Adds a **short, professional recommendation** that can go directly into your report.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **combine this Verdict Table and the Heatmap plots into a single final figure** that looks **like a polished dashboard** for your submission. It will include:\n",
    "\n",
    "* R² heatmap\n",
    "* RMSE heatmap\n",
    "* Verdict table\n",
    "* Highlighted best model\n",
    "\n",
    "It will be **ready to screenshot** for your DSML portfolio.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11340c46",
   "metadata": {},
   "source": [
    "Perfect! Let’s combine everything into a **polished “Final Metrics Dashboard”**. This will include:\n",
    "\n",
    "* **R² heatmap**\n",
    "* **RMSE heatmap**\n",
    "* **Verdict table** with best model highlighted\n",
    "* All in **one figure**, ready for portfolio submission\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Create the Dashboard**\n",
    "\n",
    "```python\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# --- Identify best model ---\n",
    "best_model_r2 = metrics_df.loc[metrics_df['Test R2'].idxmax(),'Model']\n",
    "best_model_rmse = metrics_df.loc[metrics_df['Test RMSE'].idxmin(),'Model']\n",
    "best_model = best_model_r2 if best_model_r2 == best_model_rmse else best_model_r2\n",
    "\n",
    "# --- Prepare data ---\n",
    "metrics_df_plot = metrics_df.set_index('Model')\n",
    "r2_df = metrics_df_plot[['Train R2','Test R2']]\n",
    "rmse_df = metrics_df_plot[['Train RMSE','Test RMSE']]\n",
    "\n",
    "# Verdict info\n",
    "best_metrics = metrics_df.loc[metrics_df['Model'] == best_model].copy()\n",
    "best_metrics['Recommendation'] = f\"Use {best_model} for final predictions. High Test R² and low RMSE indicate good accuracy and generalization.\"\n",
    "display_columns = ['Model', 'Test R2', 'Test RMSE', 'Recommendation']\n",
    "verdict_df = best_metrics[display_columns]\n",
    "\n",
    "# --- Plot layout ---\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 0.4], width_ratios=[1,1])\n",
    "\n",
    "# R² heatmap\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "sns.heatmap(r2_df, annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=0.5, ax=ax0)\n",
    "ax0.set_title(\"Train vs Test R²\", fontsize=14)\n",
    "for tick in ax0.get_yticklabels():\n",
    "    if tick.get_text() == best_model:\n",
    "        tick.set_weight('bold')\n",
    "        tick.set_color('red')\n",
    "\n",
    "# RMSE heatmap\n",
    "ax1 = fig.add_subplot(gs[0,1])\n",
    "sns.heatmap(rmse_df, annot=True, fmt=\".0f\", cmap=\"Oranges_r\", linewidths=0.5, ax=ax1)\n",
    "ax1.set_title(\"Train vs Test RMSE\", fontsize=14)\n",
    "for tick in ax1.get_yticklabels():\n",
    "    if tick.get_text() == best_model:\n",
    "        tick.set_weight('bold')\n",
    "        tick.set_color('red')\n",
    "\n",
    "# Verdict Table\n",
    "ax2 = fig.add_subplot(gs[1,:])\n",
    "ax2.axis('off')\n",
    "table_data = verdict_df.values\n",
    "col_labels = verdict_df.columns\n",
    "table = ax2.table(cellText=table_data, colLabels=col_labels, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1,2)\n",
    "ax2.set_title(\"Final Verdict on Best Model\", fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Features of this dashboard:**\n",
    "\n",
    "1. **Left**: R² heatmap\n",
    "2. **Right**: RMSE heatmap\n",
    "3. **Bottom**: Verdict table with recommendation\n",
    "4. **Top model highlighted in bold red** in both heatmaps\n",
    "5. **Professional layout** for portfolio screenshots\n",
    "\n",
    "---\n",
    "\n",
    "This can now be **directly exported or screenshotted** for your DSML portfolio.\n",
    "\n",
    "If you want, I can also **write a short guideline text** that explains **how to interpret this dashboard** for your report or presentation.\n",
    "\n",
    "Do you want me to do that next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ed4b4",
   "metadata": {},
   "source": [
    "Perfect! Let’s do it step by step. First, I’ll give you a **short professional guideline for interpreting your dashboard**, so it’s ready for your report. Then we can move on to **deployment**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dashboard Interpretation Guidelines**\n",
    "\n",
    "1. **R² Heatmap (Left)**\n",
    "\n",
    "   * Shows **Train vs Test R²** for all models.\n",
    "   * Higher values indicate better **explained variance**.\n",
    "   * **Red-bold model** is the best performer on test set.\n",
    "\n",
    "2. **RMSE Heatmap (Right)**\n",
    "\n",
    "   * Shows **Train vs Test RMSE**.\n",
    "   * Lower values indicate better **prediction accuracy**.\n",
    "   * **Red-bold model** is the best performer on test set.\n",
    "\n",
    "3. **Verdict Table (Bottom)**\n",
    "\n",
    "   * Summarizes the **best model** according to Test R² and RMSE.\n",
    "   * Includes **short recommendation** for which model to use for final predictions.\n",
    "\n",
    "4. **Overfitting Check**\n",
    "\n",
    "   * If Train metrics are much higher than Test metrics, model may be **overfitting**.\n",
    "   * A balanced model will have **Train ≈ Test** values for both R² and RMSE.\n",
    "\n",
    "5. **Portfolio Presentation Tip**\n",
    "\n",
    "   * Use **heatmap colors + bold red highlighting** to make the dashboard visually appealing.\n",
    "   * Reference the **Verdict Table** for the final recommendation.\n",
    "\n",
    "---\n",
    "\n",
    "Once you include this text in your portfolio, anyone reading it can **instantly understand model performance and your reasoning**.\n",
    "\n",
    "---\n",
    "\n",
    "If you’re ready, we can **start the deployment part next**.\n",
    "\n",
    "For deployment, do you want:\n",
    "\n",
    "1. **Local web app using Streamlit/Gradio**?\n",
    "2. **Cloud deployment (e.g., Hugging Face Spaces, Heroku)**?\n",
    "\n",
    "This will determine the approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b3c24",
   "metadata": {},
   "source": [
    "Perfect — here’s a **quick sanity check guide** to make sure your Random Forest truly deserves to be the “best” model:\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ Check Cross-Validation Stability**\n",
    "\n",
    "* Even if the test set R² is high, see how stable the performance is across folds.\n",
    "* Use 5–10 fold CV and compare metrics (R², RMSE).\n",
    "* **Goal:** Low standard deviation across folds → model is stable.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Example for R²\n",
    "r2_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='r2')\n",
    "print(\"R² mean:\", r2_scores.mean(), \"std:\", r2_scores.std())\n",
    "```\n",
    "\n",
    "✅ If RF has higher mean **and** low std vs XGB/LGBM → it’s robust.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Compare Same Metrics Across All Models**\n",
    "\n",
    "* Collect the **test R² and RMSE** for all models (RF, XGB, LGBM, etc.).\n",
    "* Arrange them in a **table** for clarity.\n",
    "\n",
    "| Model | R² Test | RMSE Test | CV R² Mean | CV R² Std |\n",
    "| ----- | ------- | --------- | ---------- | --------- |\n",
    "| RF    | 0.76    | 3202      | 0.74       | 0.03      |\n",
    "| LGBM  | 0.74    | 3300      | 0.72       | 0.05      |\n",
    "| XGB   | 0.73    | 3350      | 0.70       | 0.06      |\n",
    "| …     | …       | …         | …          | …         |\n",
    "\n",
    "✅ The model with **highest R², lowest RMSE, and stable CV** wins.\n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Check for Overfitting**\n",
    "\n",
    "* Compare **train vs test R²**:\n",
    "\n",
    "```python\n",
    "r2_train = best_rf.score(X_train, y_train)\n",
    "r2_test = best_rf.score(X_test, y_test)\n",
    "print(\"Train R²:\", r2_train, \"Test R²:\", r2_test)\n",
    "```\n",
    "\n",
    "* If train R² is very high but test R² drops a lot → overfitting.\n",
    "* In your case, if RF train R² ≈ test R² → safe to deploy.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ Optional Quick Bootstrap Test**\n",
    "\n",
    "* Randomly resample the training data multiple times and refit RF.\n",
    "* Check **prediction intervals** for test samples.\n",
    "* Helps validate that predictions are robust to small data changes.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ Feature Importance Sanity Check**\n",
    "\n",
    "* Look at top features from RF:\n",
    "\n",
    "```python\n",
    "importances = best_rf.feature_importances_\n",
    "features = X_train.columns\n",
    "sorted(zip(features, importances), key=lambda x: x[1], reverse=True)\n",
    "```\n",
    "\n",
    "* Make sure the important features make **logical sense** (age, BMI, health conditions, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Verdict Guide**\n",
    "\n",
    "* Highest **test R²** & lowest **RMSE** ✅\n",
    "* Stable **CV metrics** ✅\n",
    "* Not heavily overfitting ✅\n",
    "* Logical feature importance ✅\n",
    "\n",
    "If RF passes all 4 → you can confidently deploy it as your final model.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **quickly create a ready-to-use comparison table/plot** for all your models based on the values you already have. This will visually confirm why RF is the best before deployment.\n",
    "\n",
    "Do you want me to do that next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caea233",
   "metadata": {},
   "source": [
    "#### 🔎 Model Comparison Insights – Linear Models\n",
    "\n",
    "#### ✅ Performance Metrics\n",
    "\n",
    "* **Linear Regression (Baseline)**:\n",
    "\n",
    "  * Train R² = 0.70 | Test R² = 0.79 | RMSE ≈ 3020\n",
    "  * Good generalization, but risk of overfitting if assumptions are violated.\n",
    "\n",
    "* **Ridge Regression**:\n",
    "\n",
    "  * Train R² = 0.70 | Test R² = 0.79 | RMSE ≈ 3022\n",
    "  * Similar performance to OLS, but coefficients are shrunk → more stable under multicollinearity.\n",
    "\n",
    "* **Lasso Regression**:\n",
    "\n",
    "  * Train R² = 0.70 | Test R² = 0.79 | RMSE ≈ 3021\n",
    "  * No feature elimination here (all 18 features retained). However, useful in case of redundant predictors.\n",
    "\n",
    "* **Elastic Net**:\n",
    "\n",
    "  * Train R² = 0.70 | Test R² = 0.79 | RMSE ≈ 3021\n",
    "  * Combines L1 and L2 regularization; results very close to Ridge/Lasso.\n",
    "\n",
    "📌 **Conclusion**: All linear models perform almost identically on this dataset. Regularization does not significantly improve test performance, suggesting multicollinearity is limited and all features are relevant.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Coefficient Shrinkage Insights\n",
    "\n",
    "* **High Impact Predictors**:\n",
    "\n",
    "  * `Any_Transplants`, `Age_Group_60+`, `Age_Group_50-59`, and `History_of_Cancer_in_Family` consistently have the largest positive coefficients across all models → strong predictors of higher premiums.\n",
    "\n",
    "* **Negative Predictors**:\n",
    "\n",
    "  * `BMI` (continuous), `Diabetes`, and `Number_of_Major_Surgeries` show negative coefficients in some models → might reduce premiums or act as controlled factors after considering other risks.\n",
    "\n",
    "* **Regularization Effect**:\n",
    "\n",
    "  * Ridge and Elastic Net slightly shrink extreme values but do not change feature importance ranking.\n",
    "  * Lasso did **not** drop any features (all coefficients remain non-zero), meaning no redundant variables strong enough to be eliminated.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Business Insights\n",
    "\n",
    "* Insurance premiums are **most sensitive to age groups (esp. 50+) and transplant history**, aligning with domain expectations.\n",
    "* BMI plays a complex role: while obesity categories increase premiums, raw BMI is negatively weighted (possible overlap with categorical BMI encoding).\n",
    "* Since all models agree on feature importance, **linear models already capture the key drivers of premium costs**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Next Steps\n",
    "\n",
    "* **Tree-Based Models** (Decision Tree, Random Forest, Gradient Boosting) → to check if non-linear relationships improve accuracy.\n",
    "* **Model Explainability** (Permutation Importance, SHAP) → to strengthen feature-level interpretation.\n",
    "* **Check Feature Engineering** → review interaction effects (e.g., Age × BMI, Chronic Diseases × Age) to see if premiums depend on combinations of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5f91d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "             Model  Train R2  Test R2  Train RMSE  Test RMSE         Verdict\n",
    "0  Linear Regression   -445.70  -396.00   130393.42  130112.58                \n",
    "1      Decision Tree     -2.85    -2.39    12099.48   12026.90                \n",
    "2      Random Forest      0.80     0.88     2792.87    2220.72  Best Candidate\n",
    "3     Gradient Boost      0.85     0.88     2363.95    2278.97                \n",
    "4            XGBoost      1.00     0.86      378.23    2456.80                \n",
    "5           LightGBM      0.87     0.88     2261.23    2239.31                \n",
    "6     Neural Network      0.66     0.75     3583.79    3275.36                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a102c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
